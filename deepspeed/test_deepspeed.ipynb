{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated memory needed for params, optim states and gradients for a:\n",
      "HW: Setup with 1 node, 1 GPU per node.\n",
      "SW: Model with 2851M total params, 32M largest layer params.\n",
      "  per CPU  |  per GPU |   Options\n",
      "   71.71GB |   0.12GB | offload_param=OffloadDeviceEnum.cpu, offload_optimizer=OffloadDeviceEnum.cpu, zero_init=1\n",
      "   71.71GB |   0.12GB | offload_param=OffloadDeviceEnum.cpu, offload_optimizer=OffloadDeviceEnum.cpu, zero_init=0\n",
      "   63.74GB |   5.43GB | offload_param=none, offload_optimizer=OffloadDeviceEnum.cpu, zero_init=1\n",
      "   63.74GB |   5.43GB | offload_param=none, offload_optimizer=OffloadDeviceEnum.cpu, zero_init=0\n",
      "    0.18GB |  47.93GB | offload_param=none, offload_optimizer=none, zero_init=1\n",
      "   15.93GB |  47.93GB | offload_param=none, offload_optimizer=none, zero_init=0\n",
      "--------------------------------\n",
      "Estimated memory needed for params, optim states and gradients for a:\n",
      "HW: Setup with 1 node, 1 GPU per node.\n",
      "SW: Model with 2851M total params.\n",
      "  per CPU  |  per GPU |   Options\n",
      "   63.74GB |   5.31GB | offload_optimizer=OffloadDeviceEnum.cpu\n",
      "   15.93GB |  53.12GB | offload_optimizer=none\n",
      "--------------------------------\n",
      "total params:         2851.60M\n",
      "largest layer params:  32.90M\n",
      "largest layer memory:    125MB\n",
      "case1 gpu memory:  49076MB\n",
      "case2 gpu memory:    125MB\n",
      "case3 gpu memory:   5564MB\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel\n",
    "from deepspeed.runtime.zero.stage3 import estimate_zero3_model_states_mem_needs_all_live\n",
    "from deepspeed.runtime.zero.stage_1_and_2 import estimate_zero2_model_states_mem_needs_all_live\n",
    "\n",
    "# 加载预训练模型\n",
    "model = AutoModel.from_pretrained(\"t5-3b\")\n",
    "\n",
    "# 估计内存需求\n",
    "estimate_zero3_model_states_mem_needs_all_live(model, num_gpus_per_node=1, num_nodes=1)\n",
    "print(\"--------------------------------\")\n",
    "estimate_zero2_model_states_mem_needs_all_live(model, num_gpus_per_node=1, num_nodes=1)\n",
    "print(\"--------------------------------\")\n",
    "\n",
    "# shared params calculated only ones\n",
    "total_params = sum(\n",
    "    {p.data_ptr(): p.numel() for p in model.parameters()}.values()\n",
    ")\n",
    "\n",
    "largest_layer_params = 0\n",
    "for m in model.modules():\n",
    "    # assuming no shared params within a single layer\n",
    "    layer_params = sum(p.numel() for p in m.parameters(recurse=False))\n",
    "    largest_layer_params = max(largest_layer_params, layer_params)\n",
    "\n",
    "largest_layer_memory = (4*largest_layer_params)\n",
    "\n",
    "total_gpus = 1\n",
    "\n",
    "case1 = largest_layer_memory + int(18*total_params/total_gpus)\n",
    "case2 = largest_layer_memory\n",
    "case3 = largest_layer_memory + int(2*total_params/total_gpus)\n",
    "\n",
    "print(f\"total params:         {total_params/1e6:6.2f}M\")\n",
    "print(f\"largest layer params: {largest_layer_params/1e6:6.2f}M\")\n",
    "print(f\"largest layer memory: {largest_layer_memory>>20:6}MB\")\n",
    "print(f\"case1 gpu memory: {(case1)>>20:6}MB\")\n",
    "print(f\"case2 gpu memory: {(case2)>>20:6}MB\")\n",
    "print(f\"case3 gpu memory: {(case3)>>20:6}MB\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepSpeed_311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
